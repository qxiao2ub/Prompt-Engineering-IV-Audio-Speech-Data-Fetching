{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d563fa54",
   "metadata": {},
   "source": [
    "# Audio and speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1450ef",
   "metadata": {},
   "source": [
    "The OpenAI API provides range of audio capabilities. If we know what to build, then we just need to call the functions/packages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2d6678",
   "metadata": {},
   "source": [
    "## Text to speech"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f450751",
   "metadata": {},
   "source": [
    "For turning text into speech, use the Audoi API ``audio/speech`` endpoint. Models compatible with this endpoint are most up-to-date GPTs (e.g., gpt-4o, gpt-4.1). We can ask the model to speak a certain way or with a certain tone of voice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a703ab",
   "metadata": {},
   "source": [
    "### Add audio to our existing application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e05f8",
   "metadata": {},
   "source": [
    "Models such as GPT-4o, and GPT-4.1 are nativbely multimodal, meaning that they can understand and generate multiple modalities as input and output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03b1277",
   "metadata": {},
   "source": [
    "If we already have a text-based LLM application with Chat completions endpoint, then we may want to add the audiio capabilities. For example, if our chat application supports text input, we can definitely add audio input and output, just include ``audoi`` in ``modalities`` array and then use an audio model, like ``gpt-4.1``."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ce69e",
   "metadata": {},
   "source": [
    "#### Audio output from model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec8147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09855cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-_C0VLSk9gAQA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bc78cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6353ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.audio.speech.create(\n",
    "    model = \"tts-1\",\n",
    "    voice = \"alloy\", # voices: alloy, echo, fable, onyx, nova, shimmer\n",
    "    input = \"Is a golder retriever a good family dog?\"\n",
    ")\n",
    "\n",
    "# save the audio content directly\n",
    "with open(\"dog.wav\",\"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180cefe4",
   "metadata": {},
   "source": [
    "## Structured Ouputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574460e5",
   "metadata": {},
   "source": [
    "As we know, JSON is one fo the most widely used formats in the world for applications to exchange the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c863a70",
   "metadata": {},
   "source": [
    "The strucuted Ouputs is the feathre that ensuures the model will always generate resopnses that adhere to our spplied JSON schema. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1cc94e",
   "metadata": {},
   "source": [
    "Because in addtion to supporting JSON schema in the API, the OpenAI API for both Python and JavaScript make it easy to define object schemas using Pydantic and Zod respectively. Below, we will see how to extract information form the unstructured text in the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292b3e86",
   "metadata": {},
   "source": [
    "#### Chain of Thought"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1233c48d",
   "metadata": {},
   "source": [
    "We want to ask the model to output an answer in a structured, step-by-step way, to guide us (i.e., the users) through the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9a9fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps=[Step(explanation='First, subtract 7 from both sides of the equation to isolate terms with x.', output='8x + 7 - 7 = -23 - 7'), Step(explanation='Simplifying both sides gives:', output='8x = -30'), Step(explanation='Now, divide both sides by 8 to solve for x.', output='x = -30 / 8'), Step(explanation='Simplify the fraction.', output='x = -15 / 4')] final_answer='-15/4'\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Step(BaseModel):\n",
    "    explanation: str\n",
    "    output: str\n",
    "\n",
    "class MathReasoning(BaseModel):\n",
    "    steps: List[Step]\n",
    "    final_answer: str\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are a helpful math tutor. Guide the user through the solution step by step. \"\n",
    "                \"Return the result strictly as JSON in the format: \"\n",
    "                \"{'steps': [{'explanation': str, 'output': str}], 'final_answer': str}\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How can I solve 8x + 7 = -23?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "# Extract JSON text\n",
    "text_output = response.choices[0].message.content.strip()\n",
    "\n",
    "# Optional: Strip markdown-style code block\n",
    "if text_output.startswith(\"```json\"):\n",
    "    text_output = text_output[7:-3].strip()\n",
    "\n",
    "# Parse JSON string into Pydantic model\n",
    "math_reasoning = MathReasoning.model_validate_json(text_output)\n",
    "\n",
    "# Print result\n",
    "print(math_reasoning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0694da6b",
   "metadata": {},
   "source": [
    "### Structured data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db22b2a9",
   "metadata": {},
   "source": [
    "We can also define structured fields to extract from unstrcutured input data, such as the research papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be3c3bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title='A Novel Transformer-Based Approach to Climate Pattern Forecasting' authors=['Jane Doe', 'John Smith'] abstract='In this paper, we present a novel transformer-based approach to climate pattern forecasting. Our method significantly improves long-range climate event prediction.' keywords=['transformer', 'climate modeling', 'time-series forecasting']\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# Define Pydantic model for structured data\n",
    "class ResearchPaperExtraction(BaseModel):\n",
    "    title: str\n",
    "    authors: List[str]\n",
    "    abstract: str\n",
    "    keywords: List[str]\n",
    "\n",
    "# Call GPT-4 to extract structured data\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",  # or \"gpt-4\", \"gpt-4-1106-preview\", etc.\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": (\n",
    "                \"You are an expert at structured data extraction. \"\n",
    "                \"You will be given unstructured text from a research paper \"\n",
    "                \"and should extract the following fields in JSON: \"\n",
    "                \"title, authors (list of names), abstract, keywords (list of terms).\"\n",
    "            )\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"\"\"\n",
    "In this paper, we present a novel transformer-based approach to climate pattern forecasting.\n",
    "The study is conducted by Jane Doe and John Smith from the Institute of Atmospheric Science.\n",
    "Our method significantly improves long-range climate event prediction.\n",
    "Keywords include transformer, climate modeling, time-series forecasting.\n",
    "\"\"\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Get model output\n",
    "text_output = response.choices[0].message.content.strip()\n",
    "\n",
    "# Parse output to Pydantic model\n",
    "research_paper = ResearchPaperExtraction.model_validate_json(text_output)\n",
    "\n",
    "# Print structured output\n",
    "print(research_paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0783a6",
   "metadata": {},
   "source": [
    "#### Moderation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acdee64",
   "metadata": {},
   "source": [
    "Using moderation, we can classify inputs on multiple categories, which is a common way of doing moderation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67cee036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is_violating=False category=None explanation_if_violating=None\n"
     ]
    }
   ],
   "source": [
    "from enum import Enum\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class Category(str, Enum):\n",
    "    violence = \"violence\"\n",
    "    sexual = \"sexual\"\n",
    "    self_harm = \"self_harm\"\n",
    "\n",
    "class ContentCompliance(BaseModel):\n",
    "    is_violating: bool\n",
    "    category: Optional[Category]\n",
    "    explanation_if_violating: Optional[str]\n",
    "\n",
    "# Call OpenAI model\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Determine if the user input violates specific guidelines and explain if they do. Respond in JSON with fields: is_violating (true/false), category (violence/sexual/self_harm/null), explanation_if_violating (string/null).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"How do I prepare for a job interview?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract and clean up the raw JSON string\n",
    "text_output = response.choices[0].message.content.strip()\n",
    "if text_output.startswith(\"```json\"):\n",
    "    text_output = text_output[7:]\n",
    "if text_output.endswith(\"```\"):\n",
    "    text_output = text_output[:-3]\n",
    "\n",
    "# Convert string to Pydantic model\n",
    "compliance = ContentCompliance.model_validate_json(text_output)\n",
    "\n",
    "# Print result\n",
    "print(compliance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c927bf51",
   "metadata": {},
   "source": [
    "## Conversation State"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cf3690",
   "metadata": {},
   "source": [
    "In the conversation state, OpenAI provides a few ways to manage conversations, which is important for preserving information across multiple messages or turns in a conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7df8418",
   "metadata": {},
   "source": [
    "##### Manually manage conversation state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e455ebc",
   "metadata": {},
   "source": [
    "While each text generation request is independent and stateless (unless we are using the Assistants API), we can still implement multi-turn conversations by providing additional messages as parameters to our text generation request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ec83dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4.1\",\n",
    "    messages = [\n",
    "        {\"role\":\"user\",\"content\":\"knock knock.\"},\n",
    "        {\"role\":\"assistant\",\"content\":\"Who's there?\"},\n",
    "        {\"role\":\"user\", \"content\": \"Tom,\"},\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c23ec0d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tom, who?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6cc07b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4.1\",\n",
    "    messages= [\n",
    "        {\"role\":\"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\":\"user\",\"content\":\"Tell me a joke.\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "660f6fa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't skeletons fight each other?\n",
      "\n",
      "They don't have the guts!\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30275cd4",
   "metadata": {},
   "source": [
    "### OpenAI APIs for conversation state "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933c2c92",
   "metadata": {},
   "source": [
    "Our APIs make it easier to mange conversation state automatically, so we do not have to do pass inputs manually with each turn of a conversation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd6707d",
   "metadata": {},
   "source": [
    "Share context across generated responses with the ``previous_response_id`` parameter. This parameter lets us chain responses and create a ghreaded conversation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67a2d8e",
   "metadata": {},
   "source": [
    "In the following example, we will ask the model to tell a joke. Separately, we will ask the model to explain why it is funny, and the model has all the necessary context to deliver a good response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1023b989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don’t skeletons fight each other?\n",
      "\n",
      "They don’t have the guts!\n",
      "Absolutely! This joke is based on a play on words—**a pun**—involving the phrase “don’t have the guts.”\n",
      "\n",
      "- **Literal meaning:** Skeletons, by definition, are just bones. They literally have no organs, including \"guts\" (internal organs).\n",
      "- **Figurative meaning:** The phrase “don’t have the guts” is commonly used to mean someone doesn’t have the courage to do something.\n",
      "- **Humor:** The joke is funny because it combines these two meanings—skeletons physically lack guts *and* therefore “don’t have the guts” (courage) to fight.\n",
      "\n",
      "So, the humor comes from the double meaning and the surprise of connecting a well-known idiom to a literal image involving skeletons.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Step 1: First message creation\n",
    "response = client.chat.completions.create(\n",
    "    model = \"gpt-4.1\",\n",
    "    messages= [\n",
    "        {\"role\":\"user\",\"content\":\"Tell me a joke.\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "# Step 2: Follow-up message with context\n",
    "second_response = client.chat.completions.create(\n",
    "    model = \"gpt-4.1\",\n",
    "    messages = [\n",
    "        {\"role\":\"user\",\"content\":\"Tell me a joke.\"},\n",
    "        {\"role\":\"assistant\",\"content\":response.choices[0].message.content},\n",
    "        {\"role\":\"user\",\"content\":\"Explain why this is funny.\"}\n",
    "    ]\n",
    ")\n",
    "print(second_response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
